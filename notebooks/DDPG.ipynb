{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DDPG.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkLwQnWQRZin"
      },
      "source": [
        "# DEEP DETERMINISTIC POLICY GRADIENTS\n",
        "## PSEUDOCODE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJswHPRbX67N"
      },
      "source": [
        "!pip3 install box2d-py\n",
        "!pip3 install gym[Box_2D]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCs0zBE1Qz4o"
      },
      "source": [
        "import os\n",
        "from collections import deque\n",
        "import torch as T\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6zbDYmIJTBC0"
      },
      "source": [
        "**OU ActionNoise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "be67LiAwS85X"
      },
      "source": [
        "class OUActionNoise:\n",
        "    '''\n",
        "    OH_NOISE-PROCESS : https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process\n",
        "    '''\n",
        "    def __init__(self, mu, sigma = 0.15, theta = 0.2, dt = 1e-2, x0 = None):\n",
        "        self.mu = mu\n",
        "        self.sigma = sigma\n",
        "        self.theta = theta\n",
        "        self.dt = dt\n",
        "        self.x0 = x0\n",
        "        self.reset()\n",
        "    \n",
        "    def __call__(self):\n",
        "        '''\n",
        "        __call__() allows the object to be called as a Function.\n",
        "        '''\n",
        "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt \\\n",
        "            + self.sigma * np.sqrt(self.dt) * np.random.normal(size = self.mu.shape)\n",
        "        self.x_prev = x\n",
        "        return x\n",
        "    \n",
        "    def reset(self):\n",
        "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRKGCzgFTSgO"
      },
      "source": [
        "**CRITIC NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jVUgS2fFTM_j"
      },
      "source": [
        "class CriticNetwork(nn.Module):\n",
        "    '''\n",
        "    Critic Network class whose input is state and action taken while the output is a single value for \n",
        "    how good the action taken was.\n",
        "    '''\n",
        "    def __init__(self, beta, input_dim, fc1_dim, fc2_dim, n_actions, name, chkpt_dir = '/content/drive/MyDrive/RL_MODELS/DDPG_LUNAR'):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.fc1_dim = fc1_dim\n",
        "        self.fc2_dim = fc2_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.save_path = os.path.join(chkpt_dir, name + '_ddpg')\n",
        "\n",
        "        self.fc1 = nn.Linear(*self.input_dim, self.fc1_dim)\n",
        "        f1 = 1 / np.sqrt(self.fc1.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
        "        nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
        "        self.norm1 = nn.LayerNorm(self.fc1_dim)\n",
        "\n",
        "        self.fc2 = nn.Linear(self.fc1_dim, self.fc2_dim)\n",
        "        f2 = 1 / np.sqrt(self.fc2.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
        "        nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
        "        self.norm2 = nn.LayerNorm(self.fc2_dim)\n",
        "\n",
        "        self.a_v = nn.Linear(self.n_actions, self.fc2_dim)\n",
        "\n",
        "        self.q = nn.Linear(self.fc2_dim, 1)\n",
        "        fq = 0.003\n",
        "        nn.init.uniform_(self.q.weight.data, -fq, fq)\n",
        "        nn.init.uniform_(self.q.bias.data, -fq, fq)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr = beta)\n",
        "        self.device = 'cuda' if T.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.to(self.device)\n",
        "    \n",
        "    def forward(self, state, action):\n",
        "        state_value = F.relu(self.norm1(self.fc1(state)))\n",
        "\n",
        "        state_value = self.norm2(self.fc2(state_value))\n",
        "        \n",
        "        action_value = F.relu(self.a_v(action))\n",
        "\n",
        "        state_action_value = F.relu(T.add(state_value, action_value))\n",
        "        state_action_value = self.q(state_action_value)\n",
        "\n",
        "        return state_action_value\n",
        "\n",
        "    def SaveCheckpoint(self):\n",
        "        print('... SAVING CHECKPOINT ...')\n",
        "        T.save(self.state_dict(), self.save_path)\n",
        "\n",
        "    def LoadCheckpoint(self):\n",
        "        print('... LOADING CHECKPOINT ...')\n",
        "        self.load_state_dict(T.load(self.save_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbZUpYUwTanY"
      },
      "source": [
        "**ACTOR NETWORK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ONBDKHDTZti"
      },
      "source": [
        "class ActorNetwork(nn.Module):\n",
        "    '''\n",
        "    Actor Network class whose input is state while the output is vector of action probabilities.\n",
        "    '''\n",
        "    def __init__(self, alpha, input_dim, fc1_dim, fc2_dim, n_actions, name, chkpt_dir = '/content/drive/MyDrive/RL_MODELS/DDPG_LUNAR'):\n",
        "        super().__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.fc1_dim = fc1_dim\n",
        "        self.fc2_dim = fc2_dim\n",
        "        self.n_actions = n_actions\n",
        "        self.save_path = os.path.join(chkpt_dir, name + '_ddpg')\n",
        "\n",
        "        self.fc1 = nn.Linear(*self.input_dim, self.fc1_dim)\n",
        "        f1 = 1 / np.sqrt(self.fc1.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.fc1.weight.data, -f1, f1)\n",
        "        nn.init.uniform_(self.fc1.bias.data, -f1, f1)\n",
        "        self.norm1 = nn.LayerNorm(self.fc1_dim)\n",
        "\n",
        "        self.fc2 = nn.Linear(self.fc1_dim, self.fc2_dim)\n",
        "        f2 = 1 / np.sqrt(self.fc2.weight.data.size()[0])\n",
        "        nn.init.uniform_(self.fc2.weight.data, -f2, f2)\n",
        "        nn.init.uniform_(self.fc2.bias.data, -f2, f2)\n",
        "        self.norm2 = nn.LayerNorm(self.fc2_dim)\n",
        "\n",
        "        self.mu = nn.Linear(self.fc2_dim, self.n_actions)\n",
        "        fmu = 0.003\n",
        "        nn.init.uniform_(self.mu.weight.data, -fmu, fmu)\n",
        "        nn.init.uniform_(self.mu.bias.data, -fmu, fmu)\n",
        "\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr = alpha)\n",
        "        self.device = 'cuda' if T.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.to(self.device)\n",
        "    \n",
        "    def forward(self, state):\n",
        "        state_action_probs = F.relu(self.norm1(self.fc1(state)))\n",
        "        state_action_probs = F.relu(self.norm2(self.fc2(state_action_probs)))\n",
        "        state_action_probs = T.tanh(self.mu(state_action_probs))\n",
        "        return state_action_probs  \n",
        "\n",
        "    def SaveCheckpoint(self):\n",
        "        print('... SAVING CHECKPOINT ...')\n",
        "        T.save(self.state_dict(), self.save_path)\n",
        "\n",
        "    def LoadCheckpoint(self):\n",
        "        print('... LOADING CHECKPOINT ...')\n",
        "        self.load_state_dict(T.load(self.save_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bz0khmw_TiMZ"
      },
      "source": [
        "**AGENT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9Akk4LxThRC"
      },
      "source": [
        "class Agent():\n",
        "    def __init__(self, alpha, beta, tau, input_dim, gamma=0.99, n_actions = 2, fc1_dim=400, fc2_dim=300, batch_size=64):\n",
        "        self.alpha = alpha\n",
        "        self.beta = beta\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.batch_size = batch_size\n",
        "        self.device = 'cuda' if T.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.memory = deque(maxlen=10000)\n",
        "\n",
        "        self.Q = CriticNetwork(self.beta, input_dim, fc1_dim, fc2_dim, n_actions, 'Critic')\n",
        "        self.Q_prime = CriticNetwork(self.beta, input_dim, fc1_dim, fc2_dim, n_actions, 'TargetCritic')\n",
        "        self.mu = ActorNetwork(self.alpha, input_dim, fc1_dim, fc2_dim, n_actions, 'Actor')\n",
        "        self.mu_prime = ActorNetwork(self.alpha, input_dim, fc1_dim, fc2_dim, n_actions, 'TargetActor')\n",
        "\n",
        "        self.noise = OUActionNoise(mu=np.zeros(n_actions))\n",
        "\n",
        "        # The initial value of target is same as the online network\n",
        "        self.UpdateNetworkParameters(tau = 1)\n",
        "\n",
        "        self.mem_cntr = 0\n",
        "    \n",
        "    def ChooseAction(self, state):\n",
        "        self.mu.zero_grad()\n",
        "        state = T.FloatTensor(state).to(self.device)\n",
        "        a_t = self.mu(state).to(self.device) + T.Tensor(self.noise()).to(self.device)\n",
        "        self.mu.train()\n",
        "        self.mem_cntr += 1\n",
        "        return a_t.cpu().detach().numpy()\n",
        "        \n",
        "    def ReplayBuffer(self, state, action, rew, state_, done):\n",
        "        state = T.FloatTensor(state).to(self.device)\n",
        "        action = T.as_tensor(action).to(self.device)\n",
        "        state_ = T.FloatTensor(state_).to(self.device)\n",
        "        rew = T.Tensor(rew).to(self.device)\n",
        "        done = T.Tensor(1 - done).to(self.device)\n",
        "        #print(state.shape, action.shape, rew.shape, done.shape)\n",
        "        self.memory.append((state, action, rew, state_, done))\n",
        "    \n",
        "    def RetrieveBatch(self):\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        #state, action, rew, state_, done = map(T.stack, zip(*batch))\n",
        "        #\"\"\"\n",
        "        try:\n",
        "          state, action, rew, state_, done = map(T.stack, zip(*batch))\n",
        "        except RuntimeError as e:\n",
        "          print(e)\n",
        "          for k, i in enumerate(batch):\n",
        "            for j in i:\n",
        "              print(k, ':', j.shape)\n",
        "        #\"\"\"\n",
        "        return state, action, rew, state_, done\n",
        "\n",
        "    def learn(self):\n",
        "        if self.mem_cntr > self.batch_size:\n",
        "          state, action, rew, state_, done = self.RetrieveBatch()\n",
        "          \n",
        "          self.Q.eval()\n",
        "          self.Q_prime.eval()\n",
        "          self.mu_prime.eval()\n",
        "\n",
        "          target_action = self.mu_prime(state_)\n",
        "          critic_value_prime = self.Q_prime(state_, target_action).to(self.device)\n",
        "          \n",
        "          y = []\n",
        "          for i in range(self.batch_size):\n",
        "              y.append(rew[i] + self.gamma * critic_value_prime[i] * done[i])\n",
        "          \n",
        "          y = T.Tensor(y).view(self.batch_size, 1).to(self.device)\n",
        "          \n",
        "          self.Q.train()\n",
        "          self.Q.optimizer.zero_grad()\n",
        "          critic_value = self.Q(state, action).to(self.device)\n",
        "          critic_loss = F.mse_loss(y, critic_value)\n",
        "          critic_loss.backward()\n",
        "          self.Q.optimizer.step()\n",
        "          self.Q.eval()\n",
        "\n",
        "          self.mu.optimizer.zero_grad()\n",
        "          a = self.mu(state).to(self.device)\n",
        "          actor_loss = -self.Q(state, a)\n",
        "          actor_loss = T.mean(actor_loss)\n",
        "          actor_loss.backward()\n",
        "          self.mu.optimizer.step()\n",
        "\n",
        "          self.UpdateNetworkParameters()\n",
        "    \n",
        "    def UpdateNetworkParameters(self, tau = None):\n",
        "        if tau is None:\n",
        "            tau = self.tau\n",
        "\n",
        "        actor_parameters = dict(self.mu.named_parameters())\n",
        "        target_actor_parameters = dict(self.mu_prime.named_parameters())\n",
        "        critic_parameters = dict(self.Q.named_parameters())\n",
        "        target_critic_parameters = dict(self.Q_prime.named_parameters())\n",
        "\n",
        "        for name in critic_parameters:\n",
        "            target_critic_parameters[name] = tau * critic_parameters[name].clone() +\\\n",
        "                 (1 - tau) * target_critic_parameters[name].clone()\n",
        "\n",
        "        self.Q_prime.load_state_dict(target_critic_parameters)\n",
        "\n",
        "        for name in actor_parameters:\n",
        "            target_actor_parameters[name] = tau * actor_parameters[name].clone() +\\\n",
        "                (1 - tau) * target_actor_parameters[name].clone()\n",
        "        \n",
        "        #self.VerifyUpload()\n",
        "\n",
        "        self.mu_prime.load_state_dict(target_actor_parameters)\n",
        "    \n",
        "    def VerifyUpload(self):\n",
        "        target_actor_params = self.mu_prime.named_parameters()\n",
        "        target_critic_params = self.Q_prime.named_parameters()\n",
        "        critic_state_dict = dict(target_critic_params)\n",
        "        actor_state_dict = dict(target_actor_params)\n",
        "        print('\\nActor Networks\\n')\n",
        "        for name, param in self.mu.named_parameters():\n",
        "            print(name, T.equal(param, actor_state_dict[name]))\n",
        "        print('\\nCritic Networks \\n')\n",
        "        for name, param in self.Q.named_parameters():\n",
        "            print(name, T.equal(param, critic_state_dict[name]))\n",
        "    \n",
        "    def SaveModel(self):\n",
        "        self.Q.SaveCheckpoint()\n",
        "        self.Q_prime.SaveCheckpoint()\n",
        "        self.mu.SaveCheckpoint()\n",
        "        self.mu_prime.SaveCheckpoint()\n",
        "        \n",
        "    def LoadModel(self):\n",
        "        self.Q.LoadCheckpoint()\n",
        "        self.Q_prime.LoadCheckpoint()\n",
        "        self.mu.LoadCheckpoint()\n",
        "        self.mu_prime.LoadCheckpoint()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tks0S4PBT1kR"
      },
      "source": [
        "Experimenting with LUNAR LANDER environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsoS55mGTqms"
      },
      "source": [
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "\n",
        "agent = Agent(alpha = 0.000025, beta = 0.00025, input_dim = [8],\\\n",
        "    tau = 0.001, batch_size = 128, fc1_dim = 400, fc2_dim = 300, n_actions = 2)\n",
        "agent.LoadModel()\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "score_history = []\n",
        "for i in range(1000):\n",
        "    done = False\n",
        "    score = 0\n",
        "    state = env.reset()\n",
        "    while not done:\n",
        "        action = agent.ChooseAction(state)\n",
        "        state_, rew, done, _ = env.step(action)\n",
        "        agent.ReplayBuffer(state, action, np.array([rew]), state_, np.array([done]))\n",
        "        agent.learn()\n",
        "        score += rew\n",
        "        state = state_\n",
        "        #env.render()\n",
        "    score_history.append(score)\n",
        "    if i and i % 50 == 0:\n",
        "        agent.SaveModel() \n",
        "\n",
        "    print('episode ', i, 'score %.2f' % score,\n",
        "          'trailing 100 games avg %.3f' % np.mean(score_history[-100:]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}