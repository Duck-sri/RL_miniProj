{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpW4jzM-wjcU"
      },
      "source": [
        "# TWIN DELAYED DEEP DETERMINISTIC POLICY GRADIENTS\n",
        "## PSEUDOCODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hy7K9-Qzwdey"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from collections import deque\n",
        "import copy\n",
        "import torch as T\n",
        "import random\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gym\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "FN5whFPowTpP"
      },
      "outputs": [],
      "source": [
        "class Actor(nn.Module):\n",
        "    def __init__(self, state_dim, action_dim, lr, max_action, chkpt_dir='/home/shrayas/PROJECT/MIT_21-22_PROJECT/TD3 IMPLEMENTATION/'):\n",
        "        super().__init__()\n",
        "\n",
        "        self.l1 = nn.Linear(state_dim, 256)\n",
        "        self.l2 = nn.Linear(256, 256)\n",
        "        self.l3 = nn.Linear(256, action_dim)\n",
        "\n",
        "        # max_action defined to fit to any environment \n",
        "        # i.e if output is servo with rotation from \n",
        "        # -180 to 180 max_action = 180 so that the \n",
        "        # output from tanh [-1, 1] is scaled to the action range.\n",
        "        self.max_action = max_action\n",
        "\n",
        "        self.save_path = os.path.join(chkpt_dir, 'Actor_TD3')\n",
        "\n",
        "        self.optimizer = T.optim.Adam(self.parameters(), lr = lr)\n",
        "        self.device = 'cuda' if T.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.to(self.device)\n",
        "    \n",
        "    def forward(self, state):\n",
        "        val = F.relu(self.l1(state))\n",
        "        val = F.relu(self.l2(val))\n",
        "\n",
        "        return self.max_action * T.tanh(self.l3(val))\n",
        "\n",
        "    def SaveCheckpoint(self):\n",
        "        print('...SAVING CHECKPOINT...')\n",
        "        T.save(self.state_dict(), self.save_path)\n",
        "    \n",
        "    def LoadCheckpoint(self):\n",
        "        print('...LAODING CHECKPOINT...')\n",
        "        self.load_state_dict(T.load(self.save_path, map_location= 'cpu'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "qPeneHtfwTpQ"
      },
      "outputs": [],
      "source": [
        "class Critic(nn.Module):\n",
        "    '''\n",
        "    There are two critics q1, q2 involved in TD3\n",
        "    '''\n",
        "    def __init__(self, state_dim, action_dim, lr, chkpt_dir='/home/shrayas/PROJECT/MIT_21-22_PROJECT/TD3 IMPLEMENTATION/'):\n",
        "        super().__init__()                                      \n",
        "\n",
        "        self.c1l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.c1l2 = nn.Linear(256, 256)\n",
        "        self.c1l3 = nn.Linear(256, 1)\n",
        "\n",
        "        self.c2l1 = nn.Linear(state_dim + action_dim, 256)\n",
        "        self.c2l2 = nn.Linear(256, 256)\n",
        "        self.c2l3 = nn.Linear(256, 1)\n",
        "\n",
        "        self.save_path = os.path.join(chkpt_dir, 'Critic TD3')\n",
        "\n",
        "        self.optimizer = T.optim.Adam(self.parameters(), lr = lr)\n",
        "        self.device = 'cuda' if T.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        sa = T.cat([state, action], 1)\n",
        "\n",
        "        q1 = F.relu(self.c1l1(sa))\n",
        "        q1 = F.relu(self.c1l2(q1))\n",
        "        q1 = self.c1l3(q1)\n",
        "\n",
        "        q2 = F.relu(self.c2l1(sa))\n",
        "        q2 = F.relu(self.c2l2(q2))\n",
        "        q2 = self.c2l3(q2)\n",
        "\n",
        "        return q1, q2\n",
        "\n",
        "    def Q1(self, state, action):\n",
        "        sa = T.cat([state, action], 1)\n",
        "\n",
        "        Q1 = F.relu(self.c1l1(sa))\n",
        "        Q1 = F.relu(self.c1l2(Q1))\n",
        "        Q1 = self.c1l3(Q1)\n",
        "\n",
        "        return Q1\n",
        "    \n",
        "    def SaveCheckpoint(self):\n",
        "        print('...Saving Checkpoint...')\n",
        "        T.save(self.state_dict(), self.save_path)\n",
        "    \n",
        "    def LoadCheckpoint(self):\n",
        "        print('...Loading Checkpoint')\n",
        "        self.load_state_dict(T.load(self.save_path, map_location= 'cpu'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t3g6XB5MwTpS"
      },
      "outputs": [],
      "source": [
        "class TD3_Agent():\n",
        "    def __init__(self, state_dim, action_dim, max_action, gamma = 0.99, tau = 5e-3, noise_clip = 0.5, policy_delay = 2, batch_size = 100):\n",
        "        '''\n",
        "        chkpt_dir - Enter the directory where the model files are present.\n",
        "        '''\n",
        "        self.chkpt_dir = '/home/shrayas/PROJECT/MIT_21-22_PROJECT/TD3 IMPLEMENTATION/' \n",
        "        self.actor = Actor(state_dim, action_dim, 1e-3, max_action, self.chkpt_dir)\n",
        "        self.target_actor = copy.deepcopy(self.actor)\n",
        "\n",
        "        self.critic = Critic(state_dim, action_dim, 1e-3, self.chkpt_dir)\n",
        "        self.target_critic = copy.deepcopy(self.critic)\n",
        "\n",
        "        self.max_action = max_action\n",
        "        self.gamma = gamma\n",
        "        self.noise = np.random.normal(0, 0.1, action_dim)\n",
        "        self.noise_clip = noise_clip\n",
        "        self.policy_delay = policy_delay\n",
        "        self.tau = tau\n",
        "        \n",
        "        self.batch_size = batch_size\n",
        "        self.device = 'cuda' if T.cuda.is_available() else 'cpu'\n",
        "        self.memory = deque(maxlen=10000)\n",
        "\n",
        "        self.mem_counter = 0\n",
        "\n",
        "    def ChooseAction(self, state):\n",
        "        self.actor.zero_grad()\n",
        "        state = T.FloatTensor(state).to(self.device)\n",
        "        a_t = self.actor(state).to(self.device) + T.Tensor(self.noise).to(self.device)\n",
        "        self.actor.train()\n",
        "        self.mem_counter += 1\n",
        "        return a_t.cpu().detach().numpy()\n",
        "\n",
        "    def ReplayBuffer(self, state, action, rew, state_, done):\n",
        "        state = T.FloatTensor(state).to(self.device)\n",
        "        action = T.as_tensor(action).to(self.device)\n",
        "        state_ = T.FloatTensor(state_).to(self.device)\n",
        "        rew = T.Tensor(rew).to(self.device)\n",
        "        done = T.Tensor(1 - done).to(self.device)\n",
        "\n",
        "        self.memory.append((state, action, rew, state_, done))\n",
        "    \n",
        "    def RetrieveBatch(self):\n",
        "        batch = random.sample(self.memory, self.batch_size)\n",
        "        try:\n",
        "            state, action, rew, state_, done = map(T.stack, zip(*batch))\n",
        "        except RuntimeError as e:\n",
        "            print(e)\n",
        "            for k, i in enumerate(batch):\n",
        "                for j in i:\n",
        "                    print(k, ':', j.shape)\n",
        "        return state, action, rew, state_, done\n",
        "    \n",
        "    def learn(self):\n",
        "        if self.mem_counter > self.batch_size:\n",
        "            state, action, rew, state_, done = self.RetrieveBatch()\n",
        "            with T.no_grad():\n",
        "                noise_clipped = T.Tensor(self.noise).clamp(-self.noise_clip,self.noise_clip).to(self.device)\n",
        "                action_ = self.target_actor(state_) + noise_clipped\n",
        "                target_q1, target_q2 = self.critic(state_, action_)\n",
        "                target_Q = T.min(target_q1, target_q2)\n",
        "                y = rew + self.gamma * done * target_Q\n",
        "\n",
        "            Q1, Q2 = self.critic(state, action)\n",
        "            critic_loss = F.mse_loss(Q1, y) + F.mse_loss(Q2, y)\n",
        "            self.critic.optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            self.critic.optimizer.step()\n",
        "\n",
        "            # Delayed Policy update\n",
        "\n",
        "            if not self.mem_counter % self.policy_delay:\n",
        "                actor_loss = -self.critic.Q1(state, self.actor(state)).mean().to(self.device)\n",
        "                self.actor.optimizer.zero_grad()\n",
        "                actor_loss.backward()\n",
        "                self.actor.optimizer.step()\n",
        "\n",
        "                for param, target_param in zip(self.critic.parameters(),self.target_critic.parameters()):\n",
        "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "                for param, target_param in zip(self.actor.parameters(),self.target_actor.parameters()):\n",
        "                    target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "    def SaveModel(self):\n",
        "        self.actor.SaveCheckpoint()\n",
        "        self.target_actor.SaveCheckpoint()\n",
        "        self.critic.SaveCheckpoint()\n",
        "        self.target_critic.SaveCheckpoint()\n",
        "        \n",
        "    def LoadModel(self):\n",
        "        self.actor.LoadCheckpoint()\n",
        "        self.target_actor.LoadCheckpoint()\n",
        "        self.critic.LoadCheckpoint()\n",
        "        self.target_critic.LoadCheckpoint()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSAMXf6EwTpU",
        "outputId": "585a3f49-b5a2-4256-fbbb-909dd0fa876b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "...LAODING CHECKPOINT...\n",
            "...LAODING CHECKPOINT...\n",
            "...Loading Checkpoint\n",
            "...Loading Checkpoint\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_22461/3273410188.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mrew\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mscore_history\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#if i and i % 50 == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"human\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    440\u001b[0m             )\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 442\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"rgb_array\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mglClearColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m~/.local/lib/python3.8/site-packages/pyglet/window/__init__.py\u001b[0m in \u001b[0;36mclear\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1346\u001b[0m         \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mThe\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0mmust\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mactive\u001b[0m \u001b[0mcontext\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mswitch_to\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1347\u001b[0m         \"\"\"\n\u001b[0;32m-> 1348\u001b[0;31m         \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglClear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_COLOR_BUFFER_BIT\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mgl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGL_DEPTH_BUFFER_BIT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "env = gym.make('LunarLanderContinuous-v2')\n",
        "state_dim = env.observation_space.shape[0]\n",
        "action_dim = env.action_space.shape[0]\n",
        "max_action = env.action_space.high[0]\n",
        "\n",
        "agent = TD3_Agent(state_dim, action_dim, max_action, batch_size = 200)\n",
        "agent.LoadModel()\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "score_history = []\n",
        "for i in range(100):\n",
        "    done = False\n",
        "    score = 0\n",
        "    state = env.reset()\n",
        "    while not done:\n",
        "        action = agent.ChooseAction(state)\n",
        "        state_, rew, done, _ = env.step(action)\n",
        "        agent.ReplayBuffer(state, action, np.array([rew]), state_, np.array([done]))\n",
        "        #agent.learn()\n",
        "        score += rew\n",
        "        state = state_\n",
        "        env.render()\n",
        "    score_history.append(score)\n",
        "    #if i and i % 50 == 0:\n",
        "    #    agent.SaveModel() \n",
        "\n",
        "    #print('episode ', i, 'score %.2f' % score,\n",
        "    #      'trailing 100 games avg %.3f' % np.mean(score_history[-100:]))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "TD3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
